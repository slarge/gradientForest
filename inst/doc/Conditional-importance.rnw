\documentclass[a4paper,11pt]{article} %  Packages %%%%%%%%%%%%%%%%%%%%%
\usepackage{natbib}
\usepackage{pstricks}
\usepackage{pst-tree}
\usepackage{fullpage}
\usepackage{color}
\usepackage[T1]{fontenc}
\usepackage{amsmath, amssymb, bm}
\usepackage{graphicx}
\usepackage{booktabs} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{empty}
%%%%%%%% Some bold maths characters supplied by package bm
\newcommand{\bb}{\boldsymbol{\beta}}
\newcommand{\bg}{\boldsymbol{\gamma}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bz}{\mathbf{z}} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %
\title{Conditional variable importance in R package \texttt{extendedForest}}
\author{Stephen J. Smith, Nick Ellis, C. Roland Pitcher}
% %%%%%%%%%%%%%% Start of document
\begin{document}
\maketitle% Generate title
\tableofcontents% Generate table of contents
% %% Setup for R
<<c1, fig=FALSE,echo=FALSE,eval=TRUE>>=
options(width=60)
options(warn=-1)
@

% \VignetteIndexEntry{Conditional variable importance}

\section{Introduction}

The \texttt{gradientForest} package was developed to analyse large numbers of potential predictor variables 
by integrating the individual results from random forest analyses over a number of species.  
The random forests for each species were produced
by the R package \texttt{extendedForest} consisting of modifications that we made to the original 
\texttt{randomForest} package \citep{Liaw2002}.  One of the major modifications made to \texttt{randomForest}
was to the method for calculating variable importance when two or more predictor variables were correlated.

Many of the predictor variables used in ecological studies are either naturally
(e.g., decreasing temperatures with water depth) or functionally (e.g., benthic
irradiance are calculated as a function of bottom depth and light attenuation)
correlated. While some  of these predictors may determine species distribution or abundance other
collinear predictors may not.
  
The random subset approach for fitting predictor variables at each node could
result in a correlated but less influential predictor standing in for more
highly influential predictors in the early splits of an individual tree
depending upon which predictor is selected in the subset. This tendency can be
lessened by increasing the subsample size of predictors for each node but the
trade-off would be an increase in correlation between trees in the forest with
concurrent increase in generalization error and a decrease in accuracy
\citetext{\citealp{Breiman2001}; see also \citealp{Gromping2009}}.

\citet{Strobl2008} have also demonstrated that the permutation method for
estimating variable importance exhibits a bias towards correlated predictor
variables. The underlying reason for this behaviour has to do with the structure
of the null hypothesis, i.e., independence between the response $Y$ and the
predictor $X_j$ being permuted, implied by the importance measure. A small value
for the importance measure would suggest that $Y$ and $X_j$ are independent but
also assumes that $X_j$ is independent of the other predictor variables $Z$ in
the model that were not permuted ($Z=X,\ldots,X_{j-1},X_{j+1},\ldots,X_p$).
Correlation between $X_j$ and $Z$ will result in an apparent increase in
importance reflecting the lack of independence between $X_j$ and $Z$ instead of
only reflecting the lack of independence between $X_j$ and $Y$.

To remedy this situation, \citet{Strobl2008} proposed a conditional
permutation approach where the values for $X_j$ in the OOB sample for each tree
are permuted within partitions of the values of the predictors in each tree that
are correlated with $X_j$. Permutation importance is calculated by passing the
OOB samples reconfigured with this permutation grid through each respective tree
in the standard way.

In this document we present the results of a simulation study demonstrating the
impact of correlation between predictor variables on determining variable importance and
how the conditional permutation method implemented in \texttt{extendedForest} reduces this
impact.

%%%%%%%%%%%%%
The simulation and output in this document were generated by running the source
through \texttt{Sweave}, using \texttt{extendedForest} version
1.4, and R version 2.11.1 (2010-05-31).
%%%%%%%%%%%%%%%%%%%%%%%%%




\section{Methods}

\subsection{Conditional permutation}

Our implementation of the method of \citet{Strobl2008} in \texttt{extendedForest} is based on the following.
For predictor $X_j$
determine all predictors, $X_i^\prime\, (i=1,\ldots,k; i\neq j\, \mathrm{and}\,
k\leq p-1 )$ that are correlated with $X_j$ above some threshold $\rho^*$. For
tree $t$, find the first $K$ split values, $s_1,\ldots,s_K$ and indices
$v_i,\ldots,v_K$ on the $X_i^\prime$. For each observation $l$ in the OOB
sample designate a grouping or partitioning label as,

\begin{equation}
g_l = \sum_{i=1}^K 2^{i-1} I(X_{v_i}^\prime<s_i)
\end{equation}

\noindent where $I(\cdot)$ is the indicator function taking value 1
if its argument is true and 0 otherwise.

Permutation of $X_j$ in the OOB sample is applied within the above groups and
calculation of the permutation importance measure proceeds as before. The
grouping labels will take at most $2^K$ different values, although some
combinations may be missing. $K$ should be chosen not so large that there are
too few observations per partition. We used a rule-of-thumb
$K=\lfloor\log_2(0.368 N_a^{\rm sites}/2)\rfloor$, which, if the sites were
uniformly distributed among partitions, would ensure at least two points per
partition.

\subsection{Simulation Study}

We used the simulation study design in \citet{Strobl2008} to demonstrate the difference 
between determining variable importance by conditional and marginal permutation.   The response
variable was set as a function of twelve predictor variables, i.e.,

$$
y_i = \beta_1 x_{i,1} + \cdots + \beta_{12} x_{i,12} + \varepsilon_i,
$$

\noindent where $\varepsilon_i \sim \mathrm{N}(0,0.5)$. The coefficients for the predictor variables
were set so that only six of the twelve were influential.


\begin{table}[h]
\centering
  \caption{Regression coefficients for linear model used in simulation.}
\begin{tabular*}{12.0cm}{@{\extracolsep{\fill}}lcccccccccccc}
 \hline
  \hline
    & \multicolumn{12}{c}{Predictor variables}  \\\cline{2-13}
    & $x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$ & $x_6$ & $x_7$ & $x_8$ & $x_9$ & $x_{10}$ & $x_{11}$ & $x_{12}$ \\
\hline
 $\beta_{j}$ & 5 & 5 & 2 & 0 & -5 & -5 & -2 & 0 & 0 & 0 & 0 & 0  \\[3pt]
 \hline
 \end{tabular*}
   \end{table}


 The correlation structure was introduced by setting the predictor variables as a sample from
 a multivariate normal distribution with a zero mean vector and covariance $\Sigma$. All predictors were defined
  to have unit variance, $\sigma_{j,j} = 1$ and only the first four predictors were block-correlated
  with $\sigma_{j,j^\prime}=0.9$ ($i \neq j^\prime \le 4$). Off-diagonal elements were set to zero for the rest
  of the predictors.  The R code used to run the simulation follows.
 
<<c2, eval=TRUE,echo=TRUE,fig=FALSE>>= 
require(extendedForest)
require(MASS)

#Set up covariance matrix

Cov <- matrix(0,12,12)
Cov[1:4,1:4] <- 0.9
diag(Cov)[] <- 1

#Coefficients for linear model

beta <- c(5,5,2,0,-5,-5,-2,0,0,0,0,0)

# Set the maximum number of partitions to compute the importance 
# from conditional permutation distribution of each variable
maxK<-c(0,2,4,6)

# Set the number of records (or sites) and the number of simulations.
nsites<- 100
nsim <- 100

imp <- array(0,dim=c(12,4,nsim))

#Simulation 

set.seed(222)

for (sim in 1:nsim) { 
  X <- mvrnorm(nsites, rep(0,12), Sigma=Cov)
  Y <- X%*%beta + rnorm(nsites,0,0.5)
  df <- cbind(Y=Y,as.data.frame(X))
  for (lev in 1:4) { 
    RF <- randomForest(Y ~ .,df, maxLevel=maxK[lev], importance=TRUE, ntree=500, corr.threshold=0.5,mtry=8)
    imp[,lev,sim] <- RF$importance[,1]
  }
}
dimnames(imp) <- list(rownames(RF$importance), as.character(maxK), NULL)
imp <- as.data.frame.table(imp)
@


\section{Results}



Marginal permutation identifies variables 1 and 2 as most important, followed by variables 3 to 7.  
even though variable 4 had no influence on the response, its correlation to variables 1 to 3 resulted in 
this variable being ranked as more important than variables 5 to 7.  

<<c3, eval=TRUE,echo=TRUE,fig=TRUE>>=
require(lattice)
names(imp) <- c("var","maxK","sim","importance")
print(bwplot(var ~ importance | ifelse(maxK=="0", "Marginal", paste("Conditional: Level",maxK,sep="=")),imp,  as.table=T))
@

Variable importance obtained for differing number of partitions for the permutation grid are presented.
Based on our rule-of-thumb the number of partitions should be set to 4 and there appears to be little difference
between the results for $K=4$ and $K=6$. In both these cases the importance of variables 1 and 2 were very
similar to those for variables 5 and 6 as expected from the linear model.  Further, variable 4 is now just 
slightly ahead of variable 7 in importance.  Apparently, this approach eliminates most but not all of effects
of correlation.  It is possible that increasing the number of partitions may reduce the importance of variable
4 even more (compare results from $K=4$ and $K=6$), however, at some point there will not be enough observations
at each partition to calculate importance. 



\addcontentsline{toc}{section}{References}
\bibliographystyle{plainnat} \bibliography{biodiversity-survey}
\end{document}
